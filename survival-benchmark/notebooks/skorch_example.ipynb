{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c11c2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection._split import _RepeatedSplits\n",
    "from skorch.callbacks import Callback\n",
    "from skorch.dataset import CVSplit, get_len\n",
    "from skorch.utils import to_numpy\n",
    "from torch import nn\n",
    "from lifelines.utils import concordance_index\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8d57998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Tong, Li, et al. \"Deep learning based feature-level integration of multi-omics data for breast cancer patients survival analysis.\" BMC medical informatics and decision making 20.1 (2020): 1-12.\n",
    "# https://github.com/tongli1210/BreastCancerSurvivalIntegration/blob/master/src/models/loss_survival.py\n",
    "def get_R_matrix(survival_time):\n",
    "    \"\"\"\n",
    "    Create an indicator matrix of risk sets, where T_j >= T_i.\n",
    "\n",
    "    Input:\n",
    "        survival_time: a Pytorch tensor that the number of rows is equal top the number of samples\n",
    "    Output:\n",
    "        indicator matrix: an indicator matrix\n",
    "    \"\"\"\n",
    "    surv_time = np.array(survival_time)\n",
    "    return (np.outer(surv_time, surv_time) >= np.square(surv_time)).astype(int).T\n",
    "\n",
    "# Adapted from Tong, Li, et al. \"Deep learning based feature-level integration of multi-omics data for breast cancer patients survival analysis.\" BMC medical informatics and decision making 20.1 (2020): 1-12.\n",
    "# https://github.com/tongli1210/BreastCancerSurvivalIntegration/blob/master/src/models/loss_survival.py\n",
    "def neg_par_log_likelihood(\n",
    "    pred,\n",
    "    survival_time,\n",
    "    survival_event,\n",
    "    sample_weight,\n",
    "    cuda=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the average Cox negative partial log-likelihood\n",
    "    Input:\n",
    "        pred: linear predictors from trained model.\n",
    "        survival_time: survival time from ground truth\n",
    "        survival_event: survival event from ground truth: 1 for event\n",
    "                and 0 for censored\n",
    "    Output:\n",
    "        cost: the survival cost to be minimized\n",
    "    \"\"\"\n",
    "    sample_weight = torch.unsqueeze(sample_weight, 1)\n",
    "    survival_event = torch.tensor(survival_event)\n",
    "    survival_time = torch.tensor(survival_time)\n",
    "    n_observed = survival_event.sum(0)\n",
    "    if not n_observed:\n",
    "        # Return zero loss if there are no events\n",
    "        # within a batch.\n",
    "        return torch.tensor(0.0)\n",
    "    R_matrix = get_R_matrix(survival_time)\n",
    "    R_matrix = torch.Tensor(R_matrix)\n",
    "    if cuda:\n",
    "        R_matrix = R_matrix.cuda()\n",
    "    risk_set_sum = R_matrix.mm(torch.exp(pred))\n",
    "    diff = pred - torch.log(risk_set_sum)\n",
    "    survival_event = torch.reshape(\n",
    "        survival_event, (survival_event.shape[0], 1)\n",
    "    )\n",
    "    sum_diff_in_observed = (\n",
    "        torch.transpose(diff * sample_weight, 0, 1).float().mm(survival_event)\n",
    "    )\n",
    "    loss = (-(sum_diff_in_observed) / n_observed).reshape((-1,))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ac0b4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ae_criterion(nn.Module):\n",
    "    def forward(self, prediction, target, sample_weight=None):\n",
    "        if not sample_weight:\n",
    "            sample_weight = torch.ones(prediction[0].shape[0])\n",
    "        mse = nn.MSELoss()\n",
    "        return neg_par_log_likelihood(\n",
    "                prediction[0],\n",
    "                np.array([str.rsplit(i, \"|\")[1] for i in target]).astype(\n",
    "                    np.float32\n",
    "                ),\n",
    "                np.array([str.rsplit(i, \"|\")[0] for i in target]).astype(\n",
    "                    np.float32\n",
    "                ),\n",
    "                sample_weight,\n",
    "        ) + mse(prediction[1], prediction[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f07846a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Base class modeling the encoder portion of an autoencoder.\n",
    "\n",
    "    Attributes:\n",
    "        input_dimension: Input size going into the encoder.\n",
    "        hidden_layer_size: Number of hidden nodes within each hidden layer of the encoder.\n",
    "        activation: Non-linear activation method to be used by the encoder.\n",
    "        hidden_layers: Number of hidden layers within the encoder.\n",
    "        embedding_dimension: Dimensionality of the final output of the encoder (i.e., the latent space).\n",
    "        encode: `torch.Sequential` module containing the full encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension,\n",
    "        hidden_layer_size=128,\n",
    "        activation=nn.PReLU,\n",
    "        hidden_layers=1,\n",
    "        embedding_dimension=64,\n",
    "        p_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        encoder = []\n",
    "        current_size = input_dimension\n",
    "        next_size = hidden_layer_size\n",
    "        for i in range(hidden_layers):\n",
    "            if i != 0:\n",
    "                current_size = next_size\n",
    "                # Slowly halve size of the AE hidden layer dimension\n",
    "                # over time until we reach the embedding dimension.\n",
    "                # Take max since we do not want the non-bottleneck\n",
    "                # layers to become smaller than the bottleneck.\n",
    "                next_size = max(int(next_size / 2), embedding_dimension)\n",
    "            encoder.append(nn.Linear(current_size, next_size))\n",
    "            encoder.append(activation())\n",
    "            encoder.append(nn.BatchNorm1d(next_size))\n",
    "            encoder.append(nn.Dropout(p_dropout))\n",
    "        if hidden_layers > 0:\n",
    "            encoder.append(nn.Linear(next_size, embedding_dimension))\n",
    "        else:\n",
    "            encoder.append(nn.Linear(input_dimension, embedding_dimension))\n",
    "        # Do not include an activation before the embedding.\n",
    "        self.encode = nn.Sequential(*encoder)\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation = activation\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Base class modeling the decoder portion of an autoencoder.\n",
    "\n",
    "    Attributes:\n",
    "        decode: `torch.Sequential` module containing the full decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, activation=nn.PReLU, p_dropout=0.0):\n",
    "        super().__init__()\n",
    "        decoder = []\n",
    "        # Build up the decoder symmetrically from the encoder.\n",
    "        for layer in encoder.encode[::-1][::4]:\n",
    "            current_size = layer.weight.shape[0]\n",
    "            next_size = layer.weight.shape[1]\n",
    "            decoder.append(nn.Linear(current_size, next_size))\n",
    "            decoder.append(activation())\n",
    "            decoder.append(nn.BatchNorm1d(next_size))\n",
    "            decoder.append(nn.Dropout(p_dropout))\n",
    "        self.decode = nn.Sequential(*(decoder[:-3]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(x)\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "    \"\"\"Base class modeling an autoencoder.\n",
    "\n",
    "    Attributes:\n",
    "        encode: `torch.Sequential` module containing the full encoder.\n",
    "        decode: `torch.Sequential` module containing the full decoder.\n",
    "        input_dimension: Input size going into the encoder.\n",
    "        hidden_layer_size: Number of hidden nodes within each hidden layer of the encoder.\n",
    "        activation: Non-linear activation method to be used by the encoder.\n",
    "        hidden_layers: Number of hidden layers within the encoder.\n",
    "        embedding_dimension: Dimensionality of the final output of the encoder (i.e., the latent space).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension,\n",
    "        hidden_layer_size=128,\n",
    "        activation=nn.PReLU,\n",
    "        hidden_layers=1,\n",
    "        embedding_dimension=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encode = Encoder(\n",
    "            input_dimension,\n",
    "            hidden_layer_size,\n",
    "            activation,\n",
    "            hidden_layers,\n",
    "            embedding_dimension,\n",
    "        )\n",
    "        self.decode = Decoder(self.encode, activation)\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation = activation\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "class HazardRegression(nn.Module):\n",
    "    \"\"\"Base class modeling a cox hazard regression problem.\n",
    "\n",
    "    Attributes:\n",
    "        input_dimension: Number of covariates to be input to the cox regression.\n",
    "        hidden_layer_size: Number of hidden nodes within each hidden layer of the regression.\n",
    "        activation: Non-linear activation to be used after each hidden layer of the regression.\n",
    "        hidden_layers: Number of hidden layers to be used within the regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension,\n",
    "        hidden_layer_size=32,\n",
    "        activation=nn.PReLU,\n",
    "        hidden_layers=0,\n",
    "        p_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hazard = []\n",
    "        current_size = input_dimension\n",
    "        for layer in range(hidden_layers):\n",
    "            next_size = hidden_layer_size\n",
    "            hazard.append(nn.Linear(current_size, next_size))\n",
    "            hazard.append(activation())\n",
    "            hazard.append(nn.BatchNorm1d(current_size))\n",
    "            hazard.append(nn.Dropout(p_dropout))\n",
    "            current_size = next_size\n",
    "        hazard.append(nn.Linear(current_size, 1, bias=False))\n",
    "        self.hazard = nn.Sequential(*hazard)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.hazard(x)\n",
    "\n",
    "\n",
    "class SupervisedCoxAE(nn.Module):\n",
    "    def __init__(self, input_dimension):\n",
    "        super().__init__()\n",
    "        self.ae = AE(input_dimension)\n",
    "        self.hazard = HazardRegression(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, decoded = self.ae(x)\n",
    "        hazard = self.hazard(encoded)\n",
    "        return hazard, x, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "274b7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.datasets import load_flchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6c3d920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_flchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2664089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[0].dropna()[[\"age\", \"creatinine\", \"kappa\"]]\n",
    "target = data[1][X.index]\n",
    "target = np.array([f\"{int(i[0])}|{i[1]}\" for i in target])\n",
    "from skorch.net import NeuralNet\n",
    "\n",
    "class BaseSurvivalNeuralNet(NeuralNet):\n",
    "    def score(self, X: np.ndarray, y: np.ndarray):\n",
    "        try:\n",
    "            concordance = concordance_index(\n",
    "                event_times=np.vstack(np.char.split(np.array(y), sep=\"|\"))[\n",
    "                    :, 1\n",
    "                ].astype(np.float32),\n",
    "                predicted_scores=np.negative(np.squeeze(self.predict(X))),\n",
    "                event_observed=np.vstack(np.char.split(np.array(y), sep=\"|\"))[\n",
    "                    :, 0\n",
    "                ].astype(int),\n",
    "            )\n",
    "        except ValueError:\n",
    "            concordance = np.nan\n",
    "        return concordance\n",
    "    \n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        return self.criterion_(y_pred, y_true)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "18b2b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1     \u001b[36m1688.5489\u001b[0m     \u001b[32m2429.6079\u001b[0m  0.1824\n",
      "      2     \u001b[36m1640.7899\u001b[0m     \u001b[32m2154.3174\u001b[0m  0.0530\n",
      "      3     \u001b[36m1562.3875\u001b[0m     \u001b[32m2028.8997\u001b[0m  0.0561\n",
      "      4     \u001b[36m1394.7387\u001b[0m     2379.1687  0.0609\n",
      "      5     \u001b[36m1048.3701\u001b[0m     3100.4685  0.0679\n",
      "      6      \u001b[36m555.0559\u001b[0m     2764.8933  0.0674\n",
      "      7      \u001b[36m196.5069\u001b[0m      \u001b[32m804.6316\u001b[0m  0.0641\n",
      "      8       \u001b[36m70.1939\u001b[0m      \u001b[32m391.5276\u001b[0m  0.0540\n",
      "      9       \u001b[36m41.8473\u001b[0m      456.0356  0.0510\n",
      "     10       \u001b[36m35.7961\u001b[0m      \u001b[32m387.7709\u001b[0m  0.0485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5892330985472448"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BaseSurvivalNeuralNet(\n",
    "    module=SupervisedCoxAE,\n",
    "    module__input_dimension=X.shape[1],\n",
    "    criterion=ae_criterion,\n",
    "    lr=0.001\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(StandardScaler(), net)\n",
    "net.fit(X.to_numpy().astype(np.float32), target)\n",
    "net.score(\n",
    "    X.to_numpy().astype(np.float32), target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc37da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
